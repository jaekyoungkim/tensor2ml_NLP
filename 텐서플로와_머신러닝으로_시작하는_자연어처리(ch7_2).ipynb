{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "텐서플로와_머신러닝으로_시작하는_자연어처리(ch7-2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzyyWTDSzzoELdH7L6Vcqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/tensor2ml_NLP/blob/main/%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%99%80_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%9C%BC%EB%A1%9C_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC(ch7_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeQSb_ylGSEi"
      },
      "outputs": [],
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bert를 활용한 한국어 개체명 인식 모델 "
      ],
      "metadata": {
        "id": "_019je4WGYlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ner (개체명 인식) 문맥을 파악해서 인명, 기관명, 지명 등과 같은 문장 또는 문서에서 특정한 의미를 가지고 있는 단어 또는 어구를 인식하는 과정을 의미\n",
        "# 객체명 인식은 버트를 호라용한 감정분 류 및 유사도 분류와 맥락은 거의 비슷\n",
        "# 이전에 다룬 접근법들은 마지막 히든 벡터값을 사용하는 접근법이었다면 객체명 인식은 문장의 모든 입력값을 개체명으로 예측\n",
        "# 모든 은닉 벡터값을 활용한다는 차이가 있음\n",
        "# naver nlp challenge 2018 데이터 활용\n",
        "# 학습 데이터 81000개, 테스트 데이터 9000개\n",
        "# 라벨은 총 30개로 구성됨"
      ],
      "metadata": {
        "id": "ocQyNzZ-Gcic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone "
      ],
      "metadata": {
        "id": "mReXR9KT-kXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "DATA_IN_PATH= './data_in/KOR'\n",
        "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"NER\", 'train.tsv')\n",
        "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"NER\", 'test.tsv')\n",
        "DATA_LABEL_PATH = os.path.join(DATA_IN_PATH, \"NER\", 'label.txt')\n",
        "\n",
        "\n",
        "def read_file(input_path):\n",
        "  \"\"\"read tst file and return words and label as list\"\"\"\n",
        "  with open(input_path, \"r\", encoding='utf-8') as f :\n",
        "    sentences = []\n",
        "    labels =[]\n",
        "    for line in f :\n",
        "      split_line  =  line.strip().split(\"\\t\")\n",
        "      sentences.append(split_line[0])\n",
        "      labels.append(split_line[1])\n",
        "    return sentences, labels"
      ],
      "metadata": {
        "id": "wyd07HClGbEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, train_labels, = read_file(DATA_TRAIN_PATH)\n",
        "test_sentences, test_labels = read_file(DATA_TEST_PATH)\n",
        "ner_setences = train_sentences +test_sentences\n",
        "ner_labels = train_labels + train_labels\n",
        "\n",
        "ner_dict = {\"sentences\" : ner_sentences, \"label\" : ner_labels}\n",
        "ner_df = pd.DataFrame(ner_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "VkoRoTMAyaOH",
        "outputId": "916c0a8a-6483-4171-e888-a8298d1967eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7b9a65a0ea14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_TRAIN_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_TEST_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mner_setences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mtest_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mner_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-8921b0514279>\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;34m\"\"\"read tst file and return words and label as list\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_in/KOR/NER/train.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8aPFTUp0-eO4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}